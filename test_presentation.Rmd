---
title: "Text summarization with the transformer"
author: "Florian John & Willie Langenberg"
date: "2023-03-16"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction
- Our goal with this project  
- Text Summarization  
- Extractive / Abstractive  

## Data
- Amazon Fine Foods
- 586 456 rows -> 10 000 rows  

## Pre-Processing
* Text cleaning  
  - To lowercase  
  - Remove special characters (non-alphanumeric)  
  - Remove numbers  
  - Rewrite contractions (e.g. can't = cannot)  
  - Remove stop words  
  - Remove words with length less than 3  

* Tokenization

## Pre-Processing: Example review

<img src="images/preprocessing_example.png" width="100%">


## Attention
* Attention in the brain
  - used when generating memories
* Attention in code
  - using attention as weights
  - weights used to link text with output

## The transformer model

<img src="images/transformer_arcitecture.png" width="40%">

## Transformer summary

<img src="images/transformer_summary.png" width="100%">

## parameters
- four encoder and decoder layers
* FFN
  - 128 input and output neuron dimensions
  - 512 dimensions inbetween
* dropout rate = 0.1

## Result of training

<img src="images/transformer_early_stopping.png" width="70%">

## The lstm model trained with 100000 data points

<img src="images/lstm_early_stopping.jpg" width="50%">

## The lstm model trained with 1000 data points 
<img src="images/local_lstm_early_stopping.png" width="50%">

## Comparison 
* lstm
  - 
  
## Conclusion
* Model & Results  

* Discussion  
  - Pre-trained model 
  -better hardware
