---
title: "Text summarization with the transformer"
author: "Florian John & Willie Langenberg"
date: "2023-03-16"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Introduction
- Our goal with this project  
- Text Summarization  
- Extractive / Abstractive  

## Data
- Amazon Fine Foods
- 586 456 rows -> 10 000 rows

## Pre-Processing
Text cleaning  
- To lowercase  
- Remove special characters (non-alphanumeric)  
- Remove numbers  
- Rewrite contractions (e.g. can't = cannot)  
- Remove stop words  
- Remove words with length less than 3  

Tokenization

## Pre-Processing: Example review

<img src="images/preprocessing_example.png" width="100%">


## Attention
* Attention in the brain
  - used when generating memories
* Attention in code
  - using attention as weights
  - weights used to link text with output

## The transformer model

<img src="images/transformer_arcitecture.png" width="40%">

## Transformer summary

<img src="images/transformer_summary.png" width="100%">

## result of training

<img src="images/transformer_early_stopping.png" width="70%">

## the lstm model trained with 100000 data points

<img src="images/lstm_early_stopping.jpg" width="50%">

## the lstm model trained with 1000 data points 
<img src="images/local_lstm_early_stopping.png" width="50%">

## comparison 
* lstm
  - 
  
## conclusion